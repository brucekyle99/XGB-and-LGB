{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing standard libraries import numpy as np \n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame \n",
    "\n",
    "#import lightgbm and xgboost import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "\n",
    "#loading our training dataset 'adult.csv' with name 'data' using pandas \n",
    "data=pd.read_csv('adult.csv',header=None) \n",
    "\n",
    "#Assigning names to the columns \n",
    "data.columns=['age','workclass','fnlwgt','education','education-num',\n",
    "              'marital_Status','occupation','relationship','race','sex',\n",
    "              'capital_gain','capital_loss','hours_per_week','native_country','Income'] \n",
    "\n",
    "#glimpse of the dataset \n",
    "data.head() \n",
    "\n",
    "# Label Encoding our target variable \n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "l=LabelEncoder() \n",
    "l.fit(data.Income) \n",
    "\n",
    "l.classes_data.Income=Series(l.transform(data.Income))  \n",
    "\n",
    "#label encoding our target variable \n",
    "data.Income.value_counts() \n",
    "\n",
    "\n",
    "\n",
    "#One Hot Encoding of the Categorical features \n",
    "one_hot_workclass=pd.get_dummies(data.workclass) \n",
    "one_hot_education=pd.get_dummies(data.education) \n",
    "one_hot_marital_Status=pd.get_dummies(data.marital_Status) \n",
    "one_hot_occupation=pd.get_dummies(data.occupation)\n",
    "one_hot_relationship=pd.get_dummies(data.relationship) \n",
    "one_hot_race=pd.get_dummies(data.race) \n",
    "one_hot_sex=pd.get_dummies(data.sex) \n",
    "one_hot_native_country=pd.get_dummies(data.native_country) \n",
    "\n",
    "#removing categorical features \n",
    "data.drop(['workclass','education','marital_Status','occupation',\n",
    "           'relationship','race','sex','native_country'],axis=1,inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "#Merging one hot encoded features with our dataset 'data' \n",
    "data=pd.concat([data,one_hot_workclass,one_hot_education,one_hot_marital_Status,\n",
    "                one_hot_occupation,one_hot_relationship,one_hot_race,\n",
    "                one_hot_sex,one_hot_native_country],axis=1) \n",
    "\n",
    "#removing dulpicate columns \n",
    " _, i = np.unique(data.columns, return_index=True) \n",
    "data=data.iloc[:, i] \n",
    "\n",
    "#Here our target variable is 'Income' with values as 1 or 0.  #Separating our data into features dataset x and our target dataset y x=data.drop('Income',axis=1) \n",
    "y=data.Income \n",
    "\n",
    "\n",
    "\n",
    "#Imputing missing values in our target variable \n",
    "y.fillna(y.mode()[0],inplace=True) \n",
    "\n",
    "#Now splitting our dataset into test and train \n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGB\n",
    "#The data is stored in a DMatrix object #label is used to define our outcome variable\n",
    "dtrain=xgb.DMatrix(x_train,label=y_train)\n",
    "dtest=xgb.DMatrix(x_test)#setting parameters for xgboost\n",
    "parameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}\n",
    "#training our model \n",
    "num_round=50\n",
    "from datetime import datetime \n",
    "start = datetime.now() \n",
    "xg=xgb.train(parameters,dtrain,num_round) \n",
    "stop = datetime.now()#Execution time of the model \n",
    "execution_time_xgb = stop-start \n",
    "print(execution_time_xgb)\n",
    "#datetime.timedelta( , , ) representation => (days , seconds , microseconds) #now predicting our model on test set \n",
    "ypred=xg.predict(dtest) \n",
    "print(ypred)\n",
    "\n",
    "#Converting probabilities into 1 or 0  \n",
    "for i in range(0,9769): \n",
    "    if ypred[i]>=.5:       # setting threshold to .5 \n",
    "       ypred[i]=1 \n",
    "    else: \n",
    "       ypred[i]=0  #calculating accuracy of our model \n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test,ypred) \n",
    "print(accuracy_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LGB\n",
    "\n",
    "train_data=lgb.Dataset(x_train,label=y_train)\n",
    "\n",
    "# setting parameters for lightgbm\n",
    "param = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "param['metric'] = ['auc', 'binary_logloss']\n",
    "#Here we have set max_depth in xgb and LightGBM to 7 to have a fair comparison between the two.#training our model using light gbm\n",
    "num_round=50\n",
    "start=datetime.now()\n",
    "lgbm=lgb.train(param,train_data,num_round)\n",
    "stop=datetime.now()\n",
    "#Execution time of the model\n",
    "execution_time_lgbm = stop-start\n",
    "print(execution_time_lgbm)\n",
    "\n",
    "#predicting on test set\n",
    "ypred2=lgbm.predict(x_test)\n",
    "print(ypred2[0:5])  \n",
    "\n",
    "# showing first 5 predictions#converting probabilities into 0 or 1\n",
    "for i in range(0,9769):    \n",
    "    if ypred2[i]>=.5:       \n",
    "        # setting threshold to .5\n",
    "       ypred2[i]=1\n",
    "    else:  \n",
    "       ypred2[i]=0#calculating accuracy\n",
    "\n",
    "accuracy_lgbm = accuracy_score(ypred2,y_test)\n",
    "accuracy_lgbm\n",
    "\n",
    "\n",
    "y_test.value_counts()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#calculating roc_auc_score for xgboost\n",
    "auc_xgb =  roc_auc_score(y_test,ypred)\n",
    "print(auc_xgb)\n",
    "#calculating roc_auc_score for light gbm. \n",
    "auc_lgbm = roc_auc_score(y_test,ypred2)\n",
    "auc_lgbm comparison_dict = {'accuracy score':(accuracy_lgbm,accuracy_xgb),'auc score':(auc_lgbm,auc_xgb),'execution time':(execution_time_lgbm,execution_time_xgb)}\n",
    "#Creating a dataframe ‘comparison_df’ for comparing the performance of Lightgbm and xgb. \n",
    "comparison_df = DataFrame(comparison_dict) \n",
    "comparison_df.index= ['LightGBM','xgboost'] \n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
